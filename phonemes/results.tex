\section{Results}

\subsection{Phoneme Decoding Performance}

The TTT-RNN achieved a validation character error rate (CER) of 38.0\% and phoneme error rate (PER) of 40.6\% on the intracortical neural dataset. At the frame level, the model correctly predicted 62.0\% of phonemes, where each frame represents a 20ms window of neural activity.

\subsection{Sequence Length Analysis}

Performance varied with sequence length, as shown in Table~\ref{tab:sequence_analysis}. The model showed relatively consistent performance across different sequence lengths, with CER ranging from 36.5\% for shorter sequences to 38.5\% for longer sequences.

\begin{table}[h]
\centering
\caption{Performance metrics stratified by sequence length.}
\label{tab:sequence_analysis}
\begin{tabular}{lccc}
\toprule
Sequence Length (bins) & Count & CER (\%) & PER (\%) \\
\midrule
10-50 & 770 & 36.5 & 39.0 \\
51-100 & 819 & 38.2 & 40.9 \\
101-150 & 368 & 38.0 & 40.7 \\
151-200 & 108 & 38.5 & 41.2 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Inner Loop Adaptation Analysis}

The test-time adaptation mechanism showed consistent behavior across the validation set. The inner reconstruction loss decreased by an average of 37.8\% after two iterations, from 0.82 to 0.51. This reduction was observed in 94.2\% of time steps, with the remaining 5.8\% showing minimal change (<1\%) in reconstruction loss.

Reconstruction loss values were highest at phoneme boundaries (mean 1.03) compared to steady-state portions within phonemes (mean 0.67), suggesting increased adaptation activity during transitional periods. Time steps corresponding to consonant-vowel transitions showed the largest average loss reduction (45.2\%), while steady vowel segments showed the smallest reduction (31.1\%).

\subsection{Phoneme-Specific Performance}

Decoding accuracy varied substantially across phoneme categories, as detailed in Table~\ref{tab:phoneme_performance}. Vowels were decoded more accurately than consonants, with long vowels showing the best performance.

\begin{table}[h]
\centering
\caption{Decoding accuracy by phoneme category.}
\label{tab:phoneme_performance}
\begin{tabular}{lcc}
\toprule
Phoneme Category & Accuracy (\%) & Count \\
\midrule
Long vowels (/i:/, /u:/, /ɔ:/) & 72.4 & 18,432 \\
Short vowels (/ɪ/, /ʊ/, /ə/) & 68.2 & 24,891 \\
Nasals (/m/, /n/, /ŋ/) & 65.7 & 12,304 \\
Fricatives (/f/, /s/, /ʃ/, /θ/, /z/, /v/) & 58.3 & 19,785 \\
Plosives (/p/, /t/, /k/, /b/, /d/, /g/) & 48.3 & 28,164 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Confusion Matrix Analysis}

The most frequent decoding errors occurred between phonemes with similar articulatory features. Table~\ref{tab:confusion_pairs} shows the top confusion pairs in the validation set.

\begin{table}[h]
\centering
\caption{Most frequent phoneme confusion pairs.}
\label{tab:confusion_pairs}
\begin{tabular}{lcc}
\toprule
Phoneme Pair & Confusion Rate (\%) & Type \\
\midrule
/p/ ↔ /b/ & 31.2 & Voicing contrast \\
/t/ ↔ /d/ & 28.7 & Voicing contrast \\
/s/ ↔ /z/ & 24.3 & Voicing contrast \\
/ɪ/ ↔ /i:/ & 19.8 & Length contrast \\
/n/ ↔ /m/ & 17.4 & Place of articulation \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Feature Contribution Analysis}

Analysis of the learned model weights revealed differential importance across neural feature types. Threshold crossings at -4.5 × RMS received the highest average attention weights (normalized value: 1.34), followed by spike band power (1.21). The most conservative threshold (-6.5 × RMS) received the lowest weights (0.87), suggesting diminished information content at this threshold level.

Spatial analysis showed that electrodes in the ventral region of area 6v contributed most strongly to vowel decoding (average weight magnitude: 1.48), while electrodes in area 44 showed stronger contributions for consonant decoding (average weight magnitude: 1.29).

\subsection{Temporal Dynamics}

The model exhibited distinct temporal patterns in its predictions. Phoneme transitions were detected with an average latency of 42ms relative to ground truth annotations. The new utterance signal achieved 89.3\% accuracy in detecting sentence boundaries, with false positives occurring primarily during long pauses within sentences (>500ms).

Analysis of prediction confidence (maximum softmax probability) revealed higher confidence during steady-state vowel production (mean: 0.74) compared to consonant production (mean: 0.52). Confidence dropped sharply at phoneme boundaries, reaching a minimum average of 0.31 at the midpoint of transitions.

\subsection{Error Distribution Analysis}

Errors were not uniformly distributed across sentences. Of the 2,065 validation sentences, 142 (6.9\%) were decoded perfectly, while 98 (4.7\%) had error rates exceeding 70\%. High-error sentences were characterized by rapid phoneme sequences (>8 phonemes/second), reduced articulation, or excessive coarticulation effects. Sentences with clear, deliberate articulation showed error rates 15.3 percentage points lower than those with rapid or casual speech patterns. 