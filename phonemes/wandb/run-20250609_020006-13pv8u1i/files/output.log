Initialized wandb run: basic_ttt_20250609_020005
Output directory: ttt_experiments/basic_ttt_20250609_020007
Neural data loader initialized with 1280 features
Data loader setup with 1280 features
Model setup: TTT_RNN created (will build on first forward pass)
Starting training...
Loading neural data...
Found 24 .mat files
Loading sentences/t12.2022.04.28_sentences.mat...
Processing 300 sentences from t12.2022.04.28_sentences.mat
Successfully processed 262 sentences from t12.2022.04.28_sentences.mat
Loading sentences/t12.2022.05.05_sentences.mat...
Processing 380 sentences from t12.2022.05.05_sentences.mat
Successfully processed 372 sentences from t12.2022.05.05_sentences.mat
Loading sentences/t12.2022.05.17_sentences.mat...
Processing 490 sentences from t12.2022.05.17_sentences.mat
Successfully processed 490 sentences from t12.2022.05.17_sentences.mat
Loading sentences/t12.2022.05.19_sentences.mat...
Processing 250 sentences from t12.2022.05.19_sentences.mat
Successfully processed 250 sentences from t12.2022.05.19_sentences.mat
Loading sentences/t12.2022.05.24_sentences.mat...
Processing 475 sentences from t12.2022.05.24_sentences.mat
Successfully processed 474 sentences from t12.2022.05.24_sentences.mat
Loading sentences/t12.2022.05.26_sentences.mat...
Processing 450 sentences from t12.2022.05.26_sentences.mat
Successfully processed 450 sentences from t12.2022.05.26_sentences.mat
Loading sentences/t12.2022.06.02_sentences.mat...
Processing 515 sentences from t12.2022.06.02_sentences.mat
Successfully processed 515 sentences from t12.2022.06.02_sentences.mat
Loading sentences/t12.2022.06.07_sentences.mat...
Processing 490 sentences from t12.2022.06.07_sentences.mat
Successfully processed 489 sentences from t12.2022.06.07_sentences.mat
Loading sentences/t12.2022.06.14_sentences.mat...
Processing 450 sentences from t12.2022.06.14_sentences.mat
Successfully processed 450 sentences from t12.2022.06.14_sentences.mat
Loading sentences/t12.2022.06.16_sentences.mat...
Processing 450 sentences from t12.2022.06.16_sentences.mat
Successfully processed 450 sentences from t12.2022.06.16_sentences.mat
Loading sentences/t12.2022.06.21_sentences.mat...
Processing 410 sentences from t12.2022.06.21_sentences.mat
Successfully processed 410 sentences from t12.2022.06.21_sentences.mat
Loading sentences/t12.2022.06.23_sentences.mat...
Processing 570 sentences from t12.2022.06.23_sentences.mat
Successfully processed 570 sentences from t12.2022.06.23_sentences.mat
Loading sentences/t12.2022.06.28_sentences.mat...
Processing 450 sentences from t12.2022.06.28_sentences.mat
Successfully processed 450 sentences from t12.2022.06.28_sentences.mat
Loading sentences/t12.2022.07.05_sentences.mat...
Processing 450 sentences from t12.2022.07.05_sentences.mat
Successfully processed 450 sentences from t12.2022.07.05_sentences.mat
Loading sentences/t12.2022.07.14_sentences.mat...
Processing 490 sentences from t12.2022.07.14_sentences.mat
Successfully processed 490 sentences from t12.2022.07.14_sentences.mat
Loading sentences/t12.2022.07.21_sentences.mat...
Processing 490 sentences from t12.2022.07.21_sentences.mat
Successfully processed 490 sentences from t12.2022.07.21_sentences.mat
Loading sentences/t12.2022.07.27_sentences.mat...
Processing 490 sentences from t12.2022.07.27_sentences.mat
Successfully processed 490 sentences from t12.2022.07.27_sentences.mat
Loading sentences/t12.2022.07.29_sentences.mat...
Processing 240 sentences from t12.2022.07.29_sentences.mat
Successfully processed 240 sentences from t12.2022.07.29_sentences.mat
Loading sentences/t12.2022.08.02_sentences.mat...
Processing 490 sentences from t12.2022.08.02_sentences.mat
Successfully processed 490 sentences from t12.2022.08.02_sentences.mat
Loading sentences/t12.2022.08.11_sentences.mat...
Processing 410 sentences from t12.2022.08.11_sentences.mat
Successfully processed 410 sentences from t12.2022.08.11_sentences.mat
Loading sentences/t12.2022.08.13_sentences.mat...
Processing 410 sentences from t12.2022.08.13_sentences.mat
Successfully processed 410 sentences from t12.2022.08.13_sentences.mat
Loading sentences/t12.2022.08.18_sentences.mat...
Error loading sentences/t12.2022.08.18_sentences.mat: Unable to synchronously open file (file signature not found)
Loading sentences/t12.2022.08.23_sentences.mat...
Processing 610 sentences from t12.2022.08.23_sentences.mat
Successfully processed 610 sentences from t12.2022.08.23_sentences.mat
Loading sentences/t12.2022.08.25_sentences.mat...
Processing 610 sentences from t12.2022.08.25_sentences.mat
Successfully processed 610 sentences from t12.2022.08.25_sentences.mat
Loaded 10322 total sentences from 24 files
Loaded 10322 sentences
Average sentence length: 76.9 time steps
Feature range: [0.000, 1879867264.000]
Train: 8257 sentences, Val: 2065 sentences
/nlp/scr/ethanhsu/miniconda3/envs/ttt_phoneme_env/lib/python3.9/site-packages/tensorflow/python/data/ops/structured_function.py:256: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.
  warnings.warn(
Data loading complete!
Could not count model parameters: Weights for model inner_encoder have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.

Epoch 1/20
Training...
Model has 1,982,248 trainable parameters
Computing initial validation metrics...
Initial validation - Loss: 2.1174, Acc: 0.6398
  Batch 5: loss=1.0263, grad_norm=8.0057, lr=0.000100
  Batch 10: loss=0.9655, grad_norm=5.9023, lr=0.000100
  Batch 15: loss=0.8289, grad_norm=2.5953, lr=0.000100
  Batch 20: loss=0.8167, grad_norm=1.9311, lr=0.000100
  Batch 25: loss=1.0112, grad_norm=2.0275, lr=0.000100
  Batch 30: loss=0.8441, grad_norm=1.9186, lr=0.000100
  Batch 35: loss=0.6878, grad_norm=1.6709, lr=0.000100
  Batch 40: loss=0.8625, grad_norm=1.4049, lr=0.000100
  Batch 45: loss=0.7528, grad_norm=1.1874, lr=0.000100
  Batch 50: loss=0.6810, grad_norm=0.7446, lr=0.000099
  Batch 55: loss=0.7687, grad_norm=0.6767, lr=0.000099
  Batch 60: loss=0.8806, grad_norm=2.6405, lr=0.000099
  Batch 65: loss=0.6832, grad_norm=0.8926, lr=0.000099
  Batch 70: loss=0.5777, grad_norm=0.4810, lr=0.000099
  Batch 75: loss=0.7206, grad_norm=0.3458, lr=0.000099
  Batch 80: loss=0.7637, grad_norm=0.7025, lr=0.000098
  Batch 85: loss=0.7582, grad_norm=0.9640, lr=0.000098
  Batch 90: loss=0.8037, grad_norm=0.3860, lr=0.000098
  Batch 95: loss=0.7917, grad_norm=0.3429, lr=0.000098
  Batch 100: loss=0.8775, grad_norm=1.3484, lr=0.000098
  Batch 105: loss=0.5106, grad_norm=1.1455, lr=0.000097
  Batch 110: loss=0.7966, grad_norm=0.9794, lr=0.000097
  Batch 115: loss=0.6118, grad_norm=1.0543, lr=0.000097
  Batch 120: loss=0.7713, grad_norm=0.5767, lr=0.000097
  Batch 125: loss=0.7752, grad_norm=0.4015, lr=0.000096
  Batch 130: loss=0.7211, grad_norm=0.5815, lr=0.000096
  Batch 135: loss=0.5525, grad_norm=0.8808, lr=0.000096
  Batch 140: loss=0.6098, grad_norm=0.5363, lr=0.000095
  Batch 145: loss=0.6928, grad_norm=1.0474, lr=0.000095
  Batch 150: loss=0.7401, grad_norm=3.1411, lr=0.000095
  Batch 155: loss=0.7652, grad_norm=1.1945, lr=0.000094
  Batch 160: loss=0.7616, grad_norm=0.8830, lr=0.000094
  Batch 165: loss=0.7453, grad_norm=0.8056, lr=0.000094
  Batch 170: loss=0.8049, grad_norm=0.8711, lr=0.000093
  Batch 175: loss=0.6984, grad_norm=0.9124, lr=0.000093
  Batch 180: loss=0.7725, grad_norm=1.8152, lr=0.000092
  Batch 185: loss=0.6496, grad_norm=0.6334, lr=0.000092
  Batch 190: loss=0.7697, grad_norm=1.7170, lr=0.000092
  Batch 195: loss=0.7508, grad_norm=0.7103, lr=0.000091
  Batch 200: loss=0.8004, grad_norm=2.7979, lr=0.000091
  Batch 205: loss=0.5426, grad_norm=0.6614, lr=0.000090
  Batch 210: loss=0.7720, grad_norm=0.8030, lr=0.000090
  Batch 215: loss=0.8294, grad_norm=1.1875, lr=0.000089
  Batch 220: loss=0.6591, grad_norm=0.8496, lr=0.000089
  Batch 225: loss=0.7777, grad_norm=1.0559, lr=0.000088
  Batch 230: loss=0.8819, grad_norm=4.0807, lr=0.000088
  Batch 235: loss=0.5324, grad_norm=1.0170, lr=0.000087
  Batch 240: loss=0.7067, grad_norm=0.9695, lr=0.000087
  Batch 245: loss=0.7338, grad_norm=1.3055, lr=0.000086
  Batch 250: loss=0.7663, grad_norm=0.7031, lr=0.000086
  Batch 255: loss=0.6713, grad_norm=0.7767, lr=0.000085
Validation...
Epoch 1 completed in 2576.76s:
  Train Loss: 0.7660, Train Acc: 0.7129
  Val Loss: 0.7621, Val Acc: 0.7098
  Learning Rate: 0.000085
  New best validation loss: 0.7621
  New best validation accuracy: 0.7098

Epoch 2/20
Training...
Model has 1,982,248 trainable parameters
Computing initial validation metrics...
Initial validation - Loss: 0.7674, Acc: 0.7043
  Batch 5: loss=0.7613, grad_norm=0.8192, lr=0.000084
  Batch 10: loss=0.7116, grad_norm=0.3096, lr=0.000083
  Batch 15: loss=0.6309, grad_norm=0.6975, lr=0.000083
  Batch 20: loss=0.7220, grad_norm=0.6504, lr=0.000082
  Batch 25: loss=0.6994, grad_norm=0.5685, lr=0.000082
  Batch 30: loss=0.6187, grad_norm=1.0239, lr=0.000081
  Batch 35: loss=0.7286, grad_norm=0.5526, lr=0.000080
  Batch 40: loss=0.7209, grad_norm=0.7687, lr=0.000080
  Batch 45: loss=0.8031, grad_norm=0.8196, lr=0.000079
  Batch 50: loss=0.5267, grad_norm=0.4198, lr=0.000079
  Batch 55: loss=0.5972, grad_norm=0.5500, lr=0.000078
  Batch 60: loss=0.6063, grad_norm=0.5228, lr=0.000077
  Batch 65: loss=0.7144, grad_norm=0.7478, lr=0.000077
  Batch 70: loss=0.5462, grad_norm=0.3190, lr=0.000076
  Batch 75: loss=0.6134, grad_norm=0.3914, lr=0.000075
  Batch 80: loss=0.6333, grad_norm=0.4567, lr=0.000075
  Batch 85: loss=0.7471, grad_norm=0.3577, lr=0.000074
  Batch 90: loss=0.7273, grad_norm=0.6105, lr=0.000073
  Batch 95: loss=0.6800, grad_norm=0.5023, lr=0.000073
  Batch 100: loss=0.7936, grad_norm=0.5145, lr=0.000072
  Batch 105: loss=0.6097, grad_norm=0.5952, lr=0.000071
  Batch 110: loss=0.5347, grad_norm=1.0226, lr=0.000070
  Batch 115: loss=0.7590, grad_norm=1.3400, lr=0.000070
  Batch 120: loss=0.7466, grad_norm=1.2910, lr=0.000069
  Batch 125: loss=0.6655, grad_norm=6.5191, lr=0.000068
  Batch 130: loss=0.6944, grad_norm=1.3247, lr=0.000068
  Batch 135: loss=0.6963, grad_norm=0.6412, lr=0.000067
  Batch 140: loss=0.6539, grad_norm=2.0555, lr=0.000066
  Batch 145: loss=0.5541, grad_norm=1.0560, lr=0.000065
  Batch 150: loss=0.6789, grad_norm=3.1023, lr=0.000065
  Batch 155: loss=0.5897, grad_norm=1.9455, lr=0.000064
  Batch 160: loss=0.6473, grad_norm=1.8467, lr=0.000063
  Batch 165: loss=0.7346, grad_norm=0.5496, lr=0.000062
  Batch 170: loss=0.6880, grad_norm=0.9568, lr=0.000062
  Batch 175: loss=0.6864, grad_norm=2.8575, lr=0.000061
  Batch 180: loss=0.6351, grad_norm=1.2666, lr=0.000060
  Batch 185: loss=0.8202, grad_norm=1.6915, lr=0.000059
  Batch 190: loss=0.7245, grad_norm=1.3757, lr=0.000059
  Batch 195: loss=0.7576, grad_norm=3.4576, lr=0.000058
  Batch 200: loss=0.7119, grad_norm=0.8972, lr=0.000057
  Batch 205: loss=0.7523, grad_norm=0.8746, lr=0.000056
  Batch 210: loss=0.7810, grad_norm=3.4567, lr=0.000055
  Batch 215: loss=0.6823, grad_norm=1.1107, lr=0.000055
  Batch 220: loss=0.8034, grad_norm=2.1489, lr=0.000054
  Batch 225: loss=0.6926, grad_norm=3.3866, lr=0.000053
  Batch 230: loss=0.7429, grad_norm=1.0118, lr=0.000052
  Batch 235: loss=0.6155, grad_norm=1.5240, lr=0.000052
  Batch 240: loss=0.5161, grad_norm=0.7328, lr=0.000051
  Batch 245: loss=0.7059, grad_norm=0.5565, lr=0.000050
  Batch 250: loss=0.6480, grad_norm=0.7255, lr=0.000049
  Batch 255: loss=0.6616, grad_norm=2.5121, lr=0.000048
Validation...
Epoch 2 completed in 2568.50s:
  Train Loss: 0.6988, Train Acc: 0.7359
  Val Loss: 0.7486, Val Acc: 0.7210
  Learning Rate: 0.000048
  New best validation loss: 0.7486
  New best validation accuracy: 0.7210

Epoch 3/20
Training...
Model has 1,982,248 trainable parameters
Computing initial validation metrics...
Initial validation - Loss: 0.7543, Acc: 0.7196
  Batch 5: loss=0.9090, grad_norm=0.6763, lr=0.000047
  Batch 10: loss=0.7560, grad_norm=0.5199, lr=0.000046
  Batch 15: loss=0.7478, grad_norm=0.4044, lr=0.000046
  Batch 20: loss=0.6715, grad_norm=0.2973, lr=0.000045
  Batch 25: loss=0.6407, grad_norm=0.2714, lr=0.000044
  Batch 30: loss=0.5928, grad_norm=0.2990, lr=0.000043
  Batch 35: loss=0.7049, grad_norm=0.3505, lr=0.000042
  Batch 40: loss=0.6869, grad_norm=0.5167, lr=0.000042
  Batch 45: loss=0.5337, grad_norm=0.3417, lr=0.000041
  Batch 50: loss=0.7341, grad_norm=0.3903, lr=0.000040
  Batch 55: loss=0.7273, grad_norm=0.5213, lr=0.000039
  Batch 60: loss=0.7780, grad_norm=0.7229, lr=0.000039
  Batch 65: loss=0.7077, grad_norm=0.7349, lr=0.000038
  Batch 70: loss=0.7908, grad_norm=0.3689, lr=0.000037
  Batch 75: loss=0.6809, grad_norm=0.4062, lr=0.000036
  Batch 80: loss=0.7763, grad_norm=0.3038, lr=0.000036
  Batch 85: loss=0.5301, grad_norm=0.2809, lr=0.000035
  Batch 90: loss=0.7562, grad_norm=0.3381, lr=0.000034
  Batch 95: loss=0.5902, grad_norm=0.5031, lr=0.000033
  Batch 100: loss=0.6666, grad_norm=2.2354, lr=0.000033
  Batch 105: loss=0.5684, grad_norm=2.5614, lr=0.000032
  Batch 110: loss=0.7075, grad_norm=0.6440, lr=0.000031
  Batch 115: loss=0.7624, grad_norm=1.3598, lr=0.000031
  Batch 120: loss=0.5504, grad_norm=3.4918, lr=0.000030
  Batch 125: loss=0.7454, grad_norm=2.0071, lr=0.000029
  Batch 130: loss=0.7059, grad_norm=2.5188, lr=0.000028
  Batch 135: loss=0.5269, grad_norm=1.0535, lr=0.000028
  Batch 140: loss=0.7889, grad_norm=2.4328, lr=0.000027
  Batch 145: loss=0.7444, grad_norm=0.7810, lr=0.000026
  Batch 150: loss=0.4456, grad_norm=1.3611, lr=0.000026
  Batch 155: loss=0.7455, grad_norm=4.1515, lr=0.000025
  Batch 160: loss=0.6730, grad_norm=1.7528, lr=0.000024
  Batch 165: loss=0.6920, grad_norm=0.7589, lr=0.000024
  Batch 170: loss=0.7898, grad_norm=6.2516, lr=0.000023
  Batch 175: loss=0.6368, grad_norm=1.2425, lr=0.000022
  Batch 180: loss=0.6708, grad_norm=1.3930, lr=0.000022
  Batch 185: loss=0.6729, grad_norm=1.1514, lr=0.000021
  Batch 190: loss=0.7751, grad_norm=1.8926, lr=0.000021
  Batch 195: loss=0.7777, grad_norm=2.2243, lr=0.000020
  Batch 200: loss=0.6819, grad_norm=1.0759, lr=0.000019
  Batch 205: loss=0.7267, grad_norm=1.9764, lr=0.000019
  Batch 210: loss=0.6668, grad_norm=0.7108, lr=0.000018
  Batch 215: loss=0.6010, grad_norm=0.7981, lr=0.000018
  Batch 220: loss=0.6386, grad_norm=1.2367, lr=0.000017
  Batch 225: loss=0.7680, grad_norm=1.3444, lr=0.000016
  Batch 230: loss=0.7879, grad_norm=0.5470, lr=0.000016
  Batch 235: loss=0.6333, grad_norm=1.2693, lr=0.000015
  Batch 240: loss=0.7051, grad_norm=3.0610, lr=0.000015
  Batch 245: loss=0.7537, grad_norm=4.9861, lr=0.000014
  Batch 250: loss=0.7976, grad_norm=0.6318, lr=0.000014
  Batch 255: loss=0.7666, grad_norm=2.0293, lr=0.000013
Validation...
Epoch 3 completed in 2581.51s:
  Train Loss: 0.6851, Train Acc: 0.7428
  Val Loss: 0.7355, Val Acc: 0.7332
  Learning Rate: 0.000013
  New best validation loss: 0.7355
  New best validation accuracy: 0.7332

Epoch 4/20
Training...
Model has 1,982,248 trainable parameters
Computing initial validation metrics...
Initial validation - Loss: 0.7383, Acc: 0.7266
  Batch 5: loss=0.8167, grad_norm=0.9476, lr=0.000012
  Batch 10: loss=0.7240, grad_norm=0.8286, lr=0.000012
  Batch 15: loss=0.6491, grad_norm=0.3078, lr=0.000011
  Batch 20: loss=0.6752, grad_norm=0.4167, lr=0.000011
  Batch 25: loss=0.6278, grad_norm=0.4012, lr=0.000010
  Batch 30: loss=0.6361, grad_norm=0.5639, lr=0.000010
  Batch 35: loss=0.6578, grad_norm=0.2870, lr=0.000009
  Batch 40: loss=0.5860, grad_norm=0.3951, lr=0.000009
  Batch 45: loss=0.6819, grad_norm=0.4093, lr=0.000009
  Batch 50: loss=0.6228, grad_norm=0.4822, lr=0.000008
  Batch 55: loss=0.7237, grad_norm=0.3781, lr=0.000008
  Batch 60: loss=0.5769, grad_norm=0.3279, lr=0.000007
  Batch 65: loss=0.5922, grad_norm=0.3506, lr=0.000007
  Batch 70: loss=0.7730, grad_norm=0.4055, lr=0.000007
  Batch 75: loss=0.6919, grad_norm=0.2845, lr=0.000006
  Batch 80: loss=0.5938, grad_norm=0.3082, lr=0.000006
  Batch 85: loss=0.8560, grad_norm=0.4631, lr=0.000006
  Batch 90: loss=0.6097, grad_norm=0.4548, lr=0.000005
  Batch 95: loss=0.5525, grad_norm=0.5728, lr=0.000005
  Batch 100: loss=0.5585, grad_norm=0.3366, lr=0.000005
  Batch 105: loss=0.6370, grad_norm=0.6682, lr=0.000004
  Batch 110: loss=0.6184, grad_norm=0.5534, lr=0.000004
  Batch 115: loss=0.6362, grad_norm=1.3601, lr=0.000004
  Batch 120: loss=0.6722, grad_norm=0.7574, lr=0.000004
  Batch 125: loss=0.7518, grad_norm=2.7864, lr=0.000003
  Batch 130: loss=0.4346, grad_norm=2.0757, lr=0.000003
  Batch 135: loss=0.6378, grad_norm=1.8127, lr=0.000003
  Batch 140: loss=0.5610, grad_norm=1.6081, lr=0.000003
  Batch 145: loss=0.4559, grad_norm=1.1826, lr=0.000003
  Batch 150: loss=0.5787, grad_norm=1.6225, lr=0.000002
  Batch 155: loss=0.6794, grad_norm=2.0689, lr=0.000002
  Batch 160: loss=0.7571, grad_norm=0.7709, lr=0.000002
  Batch 165: loss=0.6102, grad_norm=1.0104, lr=0.000002
  Batch 170: loss=0.7018, grad_norm=1.2299, lr=0.000002
  Batch 175: loss=0.6892, grad_norm=2.7800, lr=0.000002
  Batch 180: loss=0.6329, grad_norm=0.5733, lr=0.000001
  Batch 185: loss=0.6938, grad_norm=0.6683, lr=0.000001
  Batch 190: loss=0.7919, grad_norm=1.6964, lr=0.000001
  Batch 195: loss=0.6385, grad_norm=1.0622, lr=0.000001
  Batch 200: loss=0.6631, grad_norm=0.8989, lr=0.000001
  Batch 205: loss=0.6664, grad_norm=1.3449, lr=0.000001
  Batch 210: loss=0.6653, grad_norm=1.3184, lr=0.000001
  Batch 215: loss=0.7687, grad_norm=1.7220, lr=0.000001
  Batch 220: loss=0.4645, grad_norm=0.4431, lr=0.000001
  Batch 225: loss=0.6836, grad_norm=1.2887, lr=0.000001
  Batch 230: loss=0.6987, grad_norm=1.6173, lr=0.000001
  Batch 235: loss=0.6718, grad_norm=0.9291, lr=0.000001
  Batch 240: loss=0.6675, grad_norm=0.5191, lr=0.000001
  Batch 245: loss=0.7098, grad_norm=1.5145, lr=0.000001
  Batch 250: loss=0.7017, grad_norm=1.0190, lr=0.000001
  Batch 255: loss=0.6707, grad_norm=0.5732, lr=0.000001
Validation...
Epoch 4 completed in 2611.13s:
  Train Loss: 0.6735, Train Acc: 0.7486
  Val Loss: 0.7354, Val Acc: 0.7343
  Learning Rate: 0.000001
  New best validation loss: 0.7354
  New best validation accuracy: 0.7343

Epoch 5/20
Training...
Model has 1,982,248 trainable parameters
Computing initial validation metrics...
Initial validation - Loss: 0.7354, Acc: 0.7345
  Batch 5: loss=0.7176, grad_norm=3.9750, lr=0.000001
  Batch 10: loss=0.7376, grad_norm=2.6680, lr=0.000001
  Batch 15: loss=0.6842, grad_norm=2.0923, lr=0.000001
  Batch 20: loss=0.6996, grad_norm=1.4176, lr=0.000001
  Batch 25: loss=0.6903, grad_norm=1.5784, lr=0.000001
  Batch 30: loss=0.6722, grad_norm=0.6733, lr=0.000001
  Batch 35: loss=0.9278, grad_norm=1.7587, lr=0.000001
  Batch 40: loss=0.8066, grad_norm=1.2183, lr=0.000001
  Batch 45: loss=0.6642, grad_norm=0.6143, lr=0.000001
  Batch 50: loss=0.5824, grad_norm=0.4816, lr=0.000001
  Batch 55: loss=0.8178, grad_norm=0.6736, lr=0.000001
  Batch 60: loss=0.7514, grad_norm=0.7916, lr=0.000001
  Batch 65: loss=0.5888, grad_norm=0.7621, lr=0.000001
  Batch 70: loss=0.7155, grad_norm=0.8282, lr=0.000001
  Batch 75: loss=0.5299, grad_norm=0.3927, lr=0.000001
  Batch 80: loss=0.6375, grad_norm=0.4519, lr=0.000001
  Batch 85: loss=0.4595, grad_norm=0.8862, lr=0.000001
  Batch 90: loss=0.7515, grad_norm=0.7094, lr=0.000001
  Batch 95: loss=0.7260, grad_norm=1.1328, lr=0.000001
  Batch 100: loss=0.6447, grad_norm=1.3594, lr=0.000001
  Batch 105: loss=0.5376, grad_norm=1.5940, lr=0.000001
  Batch 110: loss=0.7757, grad_norm=1.1846, lr=0.000001
  Batch 115: loss=0.7173, grad_norm=1.4119, lr=0.000001
  Batch 120: loss=0.5461, grad_norm=0.6876, lr=0.000001
  Batch 125: loss=0.5043, grad_norm=0.8869, lr=0.000001
  Batch 130: loss=0.7892, grad_norm=1.0201, lr=0.000001
  Batch 135: loss=0.5327, grad_norm=0.8201, lr=0.000001
  Batch 140: loss=0.7542, grad_norm=0.5018, lr=0.000001
  Batch 145: loss=0.5957, grad_norm=0.7738, lr=0.000001
  Batch 150: loss=0.6815, grad_norm=1.0446, lr=0.000001
  Batch 155: loss=0.7115, grad_norm=1.3034, lr=0.000001
  Batch 160: loss=0.5600, grad_norm=0.9567, lr=0.000001
  Batch 165: loss=0.5777, grad_norm=0.7916, lr=0.000001
  Batch 170: loss=0.8306, grad_norm=1.2260, lr=0.000001
  Batch 175: loss=0.5373, grad_norm=0.5752, lr=0.000001
  Batch 180: loss=0.5870, grad_norm=1.4170, lr=0.000001
  Batch 185: loss=0.6662, grad_norm=1.3808, lr=0.000001
  Batch 190: loss=0.6348, grad_norm=1.4787, lr=0.000001
  Batch 195: loss=0.6539, grad_norm=0.6338, lr=0.000001
  Batch 200: loss=0.6288, grad_norm=1.3557, lr=0.000001
  Batch 205: loss=0.5840, grad_norm=1.9280, lr=0.000001
  Batch 210: loss=0.6716, grad_norm=0.5997, lr=0.000001
  Batch 215: loss=0.6262, grad_norm=1.6839, lr=0.000001
  Batch 220: loss=0.7010, grad_norm=0.9627, lr=0.000001
  Batch 225: loss=0.6383, grad_norm=1.8897, lr=0.000001
  Batch 230: loss=0.6475, grad_norm=0.7070, lr=0.000001
  Batch 235: loss=0.7036, grad_norm=2.2183, lr=0.000001
  Batch 240: loss=0.6412, grad_norm=0.5708, lr=0.000001
  Batch 245: loss=0.5940, grad_norm=1.1002, lr=0.000001
  Batch 250: loss=0.7340, grad_norm=1.3077, lr=0.000001
  Batch 255: loss=0.7787, grad_norm=2.5694, lr=0.000001
Validation...
Epoch 5 completed in 2607.85s:
  Train Loss: 0.6780, Train Acc: 0.7444
  Val Loss: 0.7384, Val Acc: 0.7299
  Learning Rate: 0.000001
  Checkpoint saved: ttt_experiments/basic_ttt_20250609_020007/checkpoints/ckpt-1

Epoch 6/20
Training...
Model has 1,982,248 trainable parameters
Computing initial validation metrics...
Initial validation - Loss: 0.7385, Acc: 0.7296
  Batch 5: loss=0.8199, grad_norm=2.9140, lr=0.000001
  Batch 10: loss=0.6790, grad_norm=1.7633, lr=0.000001
  Batch 15: loss=0.6398, grad_norm=1.8402, lr=0.000001
  Batch 20: loss=0.5908, grad_norm=1.5105, lr=0.000001
  Batch 25: loss=0.7317, grad_norm=0.8478, lr=0.000001
  Batch 30: loss=0.7867, grad_norm=1.3975, lr=0.000001
  Batch 35: loss=0.9089, grad_norm=1.4503, lr=0.000001
  Batch 40: loss=0.6827, grad_norm=1.0235, lr=0.000001
  Batch 45: loss=0.7940, grad_norm=1.1726, lr=0.000001
  Batch 50: loss=0.6812, grad_norm=0.7033, lr=0.000001
  Batch 55: loss=0.5917, grad_norm=0.3988, lr=0.000001
  Batch 60: loss=0.6383, grad_norm=0.4772, lr=0.000001
  Batch 65: loss=0.7870, grad_norm=1.2801, lr=0.000001
  Batch 70: loss=0.7430, grad_norm=1.2315, lr=0.000001
  Batch 75: loss=0.6683, grad_norm=0.4138, lr=0.000001
  Batch 80: loss=0.8067, grad_norm=0.7672, lr=0.000001
  Batch 85: loss=0.5110, grad_norm=1.2834, lr=0.000001
  Batch 90: loss=0.5502, grad_norm=1.2771, lr=0.000001
  Batch 95: loss=0.7055, grad_norm=0.6027, lr=0.000001
  Batch 100: loss=0.7065, grad_norm=1.3534, lr=0.000001
  Batch 105: loss=0.4507, grad_norm=0.3365, lr=0.000001
  Batch 110: loss=0.6653, grad_norm=0.7191, lr=0.000001
  Batch 115: loss=0.4828, grad_norm=0.6264, lr=0.000001
  Batch 120: loss=0.7379, grad_norm=0.8266, lr=0.000001
  Batch 125: loss=0.7030, grad_norm=0.8958, lr=0.000001
  Batch 130: loss=0.6936, grad_norm=0.8939, lr=0.000001
  Batch 135: loss=0.5920, grad_norm=1.5533, lr=0.000001
  Batch 140: loss=0.7334, grad_norm=1.4439, lr=0.000001
  Batch 145: loss=0.4918, grad_norm=0.5467, lr=0.000001
  Batch 150: loss=0.5454, grad_norm=0.6873, lr=0.000001
  Batch 155: loss=0.6040, grad_norm=1.3342, lr=0.000001
  Batch 160: loss=0.7488, grad_norm=1.3558, lr=0.000001
  Batch 165: loss=0.7051, grad_norm=0.9784, lr=0.000001
  Batch 170: loss=0.6335, grad_norm=0.9440, lr=0.000001
  Batch 175: loss=0.6266, grad_norm=1.2502, lr=0.000001
  Batch 180: loss=0.6996, grad_norm=1.1598, lr=0.000001
  Batch 185: loss=0.6442, grad_norm=1.2882, lr=0.000001
  Batch 190: loss=0.7617, grad_norm=1.4314, lr=0.000001
  Batch 195: loss=0.7688, grad_norm=1.2882, lr=0.000001
  Batch 200: loss=0.7372, grad_norm=2.6151, lr=0.000001
  Batch 205: loss=0.7176, grad_norm=1.6303, lr=0.000001
  Batch 210: loss=0.6211, grad_norm=1.6653, lr=0.000001
  Batch 215: loss=0.7589, grad_norm=2.5277, lr=0.000001
  Batch 220: loss=0.6495, grad_norm=0.8192, lr=0.000001
  Batch 225: loss=0.6339, grad_norm=1.4690, lr=0.000001
  Batch 230: loss=0.6591, grad_norm=0.8133, lr=0.000001
  Batch 235: loss=0.6155, grad_norm=0.9991, lr=0.000001
  Batch 240: loss=0.6629, grad_norm=2.1527, lr=0.000001
  Batch 245: loss=0.6598, grad_norm=0.6475, lr=0.000001
  Batch 250: loss=0.7158, grad_norm=1.8444, lr=0.000001
  Batch 255: loss=0.7070, grad_norm=0.7056, lr=0.000001
Validation...
Epoch 6 completed in 2607.18s:
  Train Loss: 0.6751, Train Acc: 0.7451
  Val Loss: 0.7366, Val Acc: 0.7316
  Learning Rate: 0.000001

Epoch 7/20
Training...
Model has 1,982,248 trainable parameters
Computing initial validation metrics...
Initial validation - Loss: 0.7366, Acc: 0.7315
  Batch 5: loss=0.7380, grad_norm=2.5637, lr=0.000001
  Batch 10: loss=0.6804, grad_norm=1.5919, lr=0.000001
  Batch 15: loss=0.7479, grad_norm=2.3234, lr=0.000001
  Batch 20: loss=0.7451, grad_norm=1.9740, lr=0.000001
  Batch 25: loss=0.7809, grad_norm=1.8415, lr=0.000001
  Batch 30: loss=0.7077, grad_norm=1.3299, lr=0.000001
  Batch 35: loss=0.6299, grad_norm=0.9793, lr=0.000001
  Batch 40: loss=0.6852, grad_norm=0.5821, lr=0.000001
  Batch 45: loss=0.5681, grad_norm=0.8262, lr=0.000001
  Batch 50: loss=0.5859, grad_norm=0.4059, lr=0.000001
  Batch 55: loss=0.6425, grad_norm=0.5688, lr=0.000001
  Batch 60: loss=0.6554, grad_norm=0.5183, lr=0.000001
  Batch 65: loss=0.5919, grad_norm=0.3702, lr=0.000001
  Batch 70: loss=0.7041, grad_norm=0.8432, lr=0.000001
  Batch 75: loss=0.5429, grad_norm=0.4824, lr=0.000001
  Batch 80: loss=0.5116, grad_norm=0.5637, lr=0.000001
  Batch 85: loss=0.5939, grad_norm=0.5865, lr=0.000001
  Batch 90: loss=0.5958, grad_norm=1.0252, lr=0.000001
  Batch 95: loss=0.5766, grad_norm=0.4649, lr=0.000001
  Batch 100: loss=0.4604, grad_norm=1.6401, lr=0.000001
  Batch 105: loss=0.4948, grad_norm=0.6889, lr=0.000001
  Batch 110: loss=0.5318, grad_norm=0.4667, lr=0.000001
  Batch 115: loss=0.6944, grad_norm=1.1444, lr=0.000001
  Batch 120: loss=0.6395, grad_norm=1.6931, lr=0.000001
  Batch 125: loss=0.7367, grad_norm=1.2307, lr=0.000001
  Batch 130: loss=0.5700, grad_norm=1.7017, lr=0.000001
  Batch 135: loss=0.6355, grad_norm=0.5036, lr=0.000001
  Batch 140: loss=0.6207, grad_norm=1.9775, lr=0.000001
  Batch 145: loss=0.6670, grad_norm=0.8214, lr=0.000001
  Batch 150: loss=0.6866, grad_norm=0.9185, lr=0.000001
  Batch 155: loss=0.5707, grad_norm=1.7187, lr=0.000001
  Batch 160: loss=0.6237, grad_norm=1.0746, lr=0.000001
  Batch 165: loss=0.6498, grad_norm=1.4000, lr=0.000001
  Batch 170: loss=0.6790, grad_norm=1.6533, lr=0.000001
  Batch 175: loss=0.6794, grad_norm=0.9168, lr=0.000001
  Batch 180: loss=0.6710, grad_norm=1.1633, lr=0.000001
  Batch 185: loss=0.6315, grad_norm=1.0898, lr=0.000001
  Batch 190: loss=0.7421, grad_norm=0.6984, lr=0.000001
  Batch 195: loss=0.7372, grad_norm=1.2562, lr=0.000001
  Batch 200: loss=0.7024, grad_norm=1.8070, lr=0.000001
  Batch 205: loss=0.6200, grad_norm=2.4039, lr=0.000001
  Batch 210: loss=0.7766, grad_norm=1.6105, lr=0.000001
  Batch 215: loss=0.7243, grad_norm=0.8226, lr=0.000001
  Batch 220: loss=0.6292, grad_norm=1.6908, lr=0.000001
  Batch 225: loss=0.5869, grad_norm=1.4301, lr=0.000001
  Batch 230: loss=0.7353, grad_norm=2.7257, lr=0.000001
  Batch 235: loss=0.6223, grad_norm=1.3249, lr=0.000001
  Batch 240: loss=0.6985, grad_norm=2.4788, lr=0.000001
  Batch 245: loss=0.6571, grad_norm=1.4151, lr=0.000001
  Batch 250: loss=0.6736, grad_norm=0.9240, lr=0.000001
  Batch 255: loss=0.7632, grad_norm=1.3638, lr=0.000001
Validation...
Epoch 7 completed in 2615.38s:
  Train Loss: 0.6740, Train Acc: 0.7464
  Val Loss: 0.7381, Val Acc: 0.7280
  Learning Rate: 0.000001

Epoch 8/20
Training...
Model has 1,982,248 trainable parameters
Computing initial validation metrics...
Initial validation - Loss: 0.7383, Acc: 0.7277
  Batch 5: loss=0.7697, grad_norm=2.5380, lr=0.000001
  Batch 10: loss=0.7621, grad_norm=1.9811, lr=0.000001
  Batch 15: loss=0.6065, grad_norm=1.0365, lr=0.000001
  Batch 20: loss=0.6910, grad_norm=1.0119, lr=0.000001
  Batch 25: loss=0.6724, grad_norm=0.6830, lr=0.000001
  Batch 30: loss=0.6392, grad_norm=0.9171, lr=0.000001
  Batch 35: loss=0.7633, grad_norm=1.2468, lr=0.000001
  Batch 40: loss=0.6811, grad_norm=0.6622, lr=0.000001
  Batch 45: loss=0.8905, grad_norm=0.8174, lr=0.000001
  Batch 50: loss=0.7624, grad_norm=0.5397, lr=0.000001
  Batch 55: loss=0.8044, grad_norm=0.4228, lr=0.000001
  Batch 60: loss=0.6250, grad_norm=0.6112, lr=0.000001
  Batch 65: loss=0.8172, grad_norm=0.3730, lr=0.000001
  Batch 70: loss=0.6562, grad_norm=0.5268, lr=0.000001
  Batch 75: loss=0.6535, grad_norm=0.6785, lr=0.000001
  Batch 80: loss=0.6122, grad_norm=0.4373, lr=0.000001
  Batch 85: loss=0.5157, grad_norm=1.2557, lr=0.000001
  Batch 90: loss=0.7491, grad_norm=1.2697, lr=0.000001
  Batch 95: loss=0.6236, grad_norm=1.1836, lr=0.000001
  Batch 100: loss=0.7853, grad_norm=1.7595, lr=0.000001
  Batch 105: loss=0.6466, grad_norm=2.1008, lr=0.000001
  Batch 110: loss=0.6461, grad_norm=2.3303, lr=0.000001
  Batch 115: loss=0.5738, grad_norm=0.6947, lr=0.000001
  Batch 120: loss=0.5561, grad_norm=0.9568, lr=0.000001
  Batch 125: loss=0.5850, grad_norm=1.4322, lr=0.000001
  Batch 130: loss=0.5364, grad_norm=0.6555, lr=0.000001
  Batch 135: loss=0.5462, grad_norm=1.2218, lr=0.000001
  Batch 140: loss=0.6190, grad_norm=0.9925, lr=0.000001
  Batch 145: loss=0.6578, grad_norm=2.5278, lr=0.000001
  Batch 150: loss=0.6048, grad_norm=2.1295, lr=0.000001
  Batch 155: loss=0.7682, grad_norm=1.4612, lr=0.000001
  Batch 160: loss=0.7138, grad_norm=1.0469, lr=0.000001
  Batch 165: loss=0.5946, grad_norm=0.7806, lr=0.000001
  Batch 170: loss=0.6975, grad_norm=1.1068, lr=0.000001
  Batch 175: loss=0.6109, grad_norm=1.2020, lr=0.000001
  Batch 180: loss=0.7501, grad_norm=1.1098, lr=0.000001
  Batch 185: loss=0.5730, grad_norm=0.8799, lr=0.000001
  Batch 190: loss=0.8186, grad_norm=1.4335, lr=0.000001
  Batch 195: loss=0.6648, grad_norm=2.5102, lr=0.000001
  Batch 200: loss=0.7117, grad_norm=1.0004, lr=0.000001
  Batch 205: loss=0.6897, grad_norm=1.8307, lr=0.000001
  Batch 210: loss=0.6262, grad_norm=0.9474, lr=0.000001
  Batch 215: loss=0.7194, grad_norm=2.3005, lr=0.000001
  Batch 220: loss=0.6510, grad_norm=1.0679, lr=0.000001
  Batch 225: loss=0.6741, grad_norm=1.1013, lr=0.000001
  Batch 230: loss=0.6559, grad_norm=1.3379, lr=0.000001
  Batch 235: loss=0.6685, grad_norm=1.6364, lr=0.000001
  Batch 240: loss=0.6804, grad_norm=0.7374, lr=0.000001
  Batch 245: loss=0.7059, grad_norm=1.5887, lr=0.000001
  Batch 250: loss=0.6299, grad_norm=0.8936, lr=0.000001
  Batch 255: loss=0.5548, grad_norm=0.7847, lr=0.000001
Validation...
Epoch 8 completed in 2601.08s:
  Train Loss: 0.6780, Train Acc: 0.7452
  Val Loss: 0.7358, Val Acc: 0.7333
  Learning Rate: 0.000001

Epoch 9/20
Training...
Model has 1,982,248 trainable parameters
Computing initial validation metrics...
Initial validation - Loss: 0.7357, Acc: 0.7335
  Batch 5: loss=0.7529, grad_norm=3.6665, lr=0.000001
  Batch 10: loss=0.7773, grad_norm=2.9530, lr=0.000001
  Batch 15: loss=0.6499, grad_norm=2.2638, lr=0.000001
  Batch 20: loss=0.7290, grad_norm=1.8556, lr=0.000001
  Batch 25: loss=0.7923, grad_norm=1.4448, lr=0.000001
  Batch 30: loss=0.7303, grad_norm=1.0695, lr=0.000001
  Batch 35: loss=0.6740, grad_norm=0.9292, lr=0.000001
  Batch 40: loss=0.7399, grad_norm=1.0934, lr=0.000001
  Batch 45: loss=0.5643, grad_norm=0.5002, lr=0.000001
  Batch 50: loss=0.6537, grad_norm=0.4378, lr=0.000001
  Batch 55: loss=0.7924, grad_norm=0.6847, lr=0.000001
  Batch 60: loss=0.7303, grad_norm=1.0637, lr=0.000001
  Batch 65: loss=0.6095, grad_norm=0.5281, lr=0.000001
  Batch 70: loss=0.5510, grad_norm=0.5605, lr=0.000001
  Batch 75: loss=0.5636, grad_norm=0.4699, lr=0.000001
  Batch 80: loss=0.4945, grad_norm=0.8695, lr=0.000001
  Batch 85: loss=0.6738, grad_norm=1.7009, lr=0.000001
  Batch 90: loss=0.7007, grad_norm=1.0256, lr=0.000001
  Batch 95: loss=0.6936, grad_norm=1.1878, lr=0.000001
  Batch 100: loss=0.4470, grad_norm=0.6183, lr=0.000001
  Batch 105: loss=0.5746, grad_norm=0.9691, lr=0.000001
  Batch 110: loss=0.5496, grad_norm=0.4916, lr=0.000001
  Batch 115: loss=0.6961, grad_norm=2.2724, lr=0.000001
  Batch 120: loss=0.6880, grad_norm=1.0133, lr=0.000001
  Batch 125: loss=0.5738, grad_norm=1.6356, lr=0.000001
  Batch 130: loss=0.6767, grad_norm=0.9694, lr=0.000001
  Batch 135: loss=0.5562, grad_norm=2.0250, lr=0.000001
  Batch 140: loss=0.4764, grad_norm=0.5787, lr=0.000001
  Batch 145: loss=0.5312, grad_norm=1.8096, lr=0.000001
  Batch 150: loss=0.6511, grad_norm=0.9435, lr=0.000001
  Batch 155: loss=0.6532, grad_norm=2.6741, lr=0.000001
  Batch 160: loss=0.6001, grad_norm=2.5085, lr=0.000001
  Batch 165: loss=0.6574, grad_norm=2.0397, lr=0.000001
  Batch 170: loss=0.6571, grad_norm=1.9197, lr=0.000001
  Batch 175: loss=0.7011, grad_norm=2.1532, lr=0.000001
  Batch 180: loss=0.7259, grad_norm=2.8787, lr=0.000001
  Batch 185: loss=0.7443, grad_norm=1.5220, lr=0.000001
  Batch 190: loss=0.8066, grad_norm=2.6965, lr=0.000001
  Batch 195: loss=0.7363, grad_norm=1.8100, lr=0.000001
  Batch 200: loss=0.6225, grad_norm=1.5584, lr=0.000001
  Batch 205: loss=0.7042, grad_norm=1.6952, lr=0.000001
  Batch 210: loss=0.6562, grad_norm=1.2008, lr=0.000001
  Batch 215: loss=0.7993, grad_norm=1.7390, lr=0.000001
  Batch 220: loss=0.6913, grad_norm=2.4124, lr=0.000001
  Batch 225: loss=0.7259, grad_norm=0.7841, lr=0.000001
  Batch 230: loss=0.7071, grad_norm=1.5627, lr=0.000001
  Batch 235: loss=0.7180, grad_norm=0.8526, lr=0.000001
  Batch 240: loss=0.7102, grad_norm=0.7593, lr=0.000001
  Batch 245: loss=0.7485, grad_norm=1.0479, lr=0.000001
  Batch 250: loss=0.6770, grad_norm=1.4042, lr=0.000001
  Batch 255: loss=0.6796, grad_norm=1.4648, lr=0.000001
Validation...
Epoch 9 completed in 2600.62s:
  Train Loss: 0.6816, Train Acc: 0.7435
  Val Loss: 0.7369, Val Acc: 0.7331
  Learning Rate: 0.000001

Epoch 10/20
Training...
Model has 1,982,248 trainable parameters
Computing initial validation metrics...
Initial validation - Loss: 0.7369, Acc: 0.7332
  Batch 5: loss=0.7769, grad_norm=3.0404, lr=0.000001
  Batch 10: loss=0.7065, grad_norm=3.1583, lr=0.000001
  Batch 15: loss=0.8449, grad_norm=2.4628, lr=0.000001
  Batch 20: loss=0.7419, grad_norm=2.2492, lr=0.000001
  Batch 25: loss=0.6107, grad_norm=1.1629, lr=0.000001
  Batch 30: loss=0.7447, grad_norm=0.8158, lr=0.000001
  Batch 35: loss=0.6627, grad_norm=0.7240, lr=0.000001
  Batch 40: loss=0.8412, grad_norm=1.3703, lr=0.000001
  Batch 45: loss=0.5922, grad_norm=0.5972, lr=0.000001
  Batch 50: loss=0.5720, grad_norm=0.3995, lr=0.000001
  Batch 55: loss=0.5868, grad_norm=0.5000, lr=0.000001
  Batch 60: loss=0.6843, grad_norm=0.6628, lr=0.000001
  Batch 65: loss=0.5423, grad_norm=0.9390, lr=0.000001
  Batch 70: loss=0.7048, grad_norm=0.8945, lr=0.000001
  Batch 75: loss=0.6885, grad_norm=0.6174, lr=0.000001
  Batch 80: loss=0.6502, grad_norm=0.8165, lr=0.000001
  Batch 85: loss=0.6009, grad_norm=0.9243, lr=0.000001
  Batch 90: loss=0.6701, grad_norm=2.0660, lr=0.000001
  Batch 95: loss=0.7781, grad_norm=1.7151, lr=0.000001
  Batch 100: loss=0.6335, grad_norm=1.0686, lr=0.000001
  Batch 105: loss=0.6848, grad_norm=0.9865, lr=0.000001
  Batch 110: loss=0.6520, grad_norm=2.0919, lr=0.000001
  Batch 115: loss=0.7223, grad_norm=0.9983, lr=0.000001
  Batch 120: loss=0.5713, grad_norm=1.1866, lr=0.000001
  Batch 125: loss=0.7319, grad_norm=1.2652, lr=0.000001
  Batch 130: loss=0.5744, grad_norm=1.0916, lr=0.000001
  Batch 135: loss=0.4887, grad_norm=0.7218, lr=0.000001
  Batch 140: loss=0.6890, grad_norm=0.7525, lr=0.000001
  Batch 145: loss=0.4948, grad_norm=1.0401, lr=0.000001
  Batch 150: loss=0.5421, grad_norm=2.2057, lr=0.000001
  Batch 155: loss=0.7173, grad_norm=1.6219, lr=0.000001
  Batch 160: loss=0.6826, grad_norm=1.7998, lr=0.000001
  Batch 165: loss=0.6509, grad_norm=0.6499, lr=0.000001
  Batch 170: loss=0.6580, grad_norm=0.8354, lr=0.000001
  Batch 175: loss=0.7215, grad_norm=2.3161, lr=0.000001
  Batch 180: loss=0.6214, grad_norm=2.4532, lr=0.000001
  Batch 185: loss=0.5220, grad_norm=0.9386, lr=0.000001
  Batch 190: loss=0.7274, grad_norm=0.7381, lr=0.000001
  Batch 195: loss=0.6021, grad_norm=2.1363, lr=0.000001
  Batch 200: loss=0.7182, grad_norm=1.5416, lr=0.000001
  Batch 205: loss=0.6460, grad_norm=0.8700, lr=0.000001
  Batch 210: loss=0.6137, grad_norm=3.2017, lr=0.000001
  Batch 215: loss=0.7387, grad_norm=1.2942, lr=0.000001
  Batch 220: loss=0.5615, grad_norm=1.5747, lr=0.000001
  Batch 225: loss=0.6722, grad_norm=2.8908, lr=0.000001
  Batch 230: loss=0.6378, grad_norm=0.7287, lr=0.000001
  Batch 235: loss=0.6372, grad_norm=1.0651, lr=0.000001
  Batch 240: loss=0.7439, grad_norm=1.4938, lr=0.000001
  Batch 245: loss=0.5385, grad_norm=0.5058, lr=0.000001
  Batch 250: loss=0.5932, grad_norm=0.9800, lr=0.000001
  Batch 255: loss=0.6449, grad_norm=1.1908, lr=0.000001
Validation...
Epoch 10 completed in 2634.18s:
  Train Loss: 0.6756, Train Acc: 0.7464
  Val Loss: 0.7372, Val Acc: 0.7300
  Learning Rate: 0.000001
  Checkpoint saved: ttt_experiments/basic_ttt_20250609_020007/checkpoints/ckpt-2

Epoch 11/20
Training...
Model has 1,982,248 trainable parameters
Computing initial validation metrics...
Initial validation - Loss: 0.7374, Acc: 0.7296
  Batch 5: loss=0.7311, grad_norm=2.8705, lr=0.000001
  Batch 10: loss=0.7196, grad_norm=2.7892, lr=0.000001
  Batch 15: loss=0.7293, grad_norm=2.3128, lr=0.000001
  Batch 20: loss=0.6146, grad_norm=0.8595, lr=0.000001
  Batch 25: loss=0.7030, grad_norm=1.0067, lr=0.000001
  Batch 30: loss=0.6376, grad_norm=0.9141, lr=0.000001
  Batch 35: loss=0.6769, grad_norm=0.7950, lr=0.000001
  Batch 40: loss=0.7758, grad_norm=1.0353, lr=0.000001
  Batch 45: loss=0.6522, grad_norm=0.4715, lr=0.000001
  Batch 50: loss=0.7237, grad_norm=0.6428, lr=0.000001
  Batch 55: loss=0.6592, grad_norm=0.4068, lr=0.000001
  Batch 60: loss=0.5816, grad_norm=1.0726, lr=0.000001
  Batch 65: loss=0.7768, grad_norm=0.7721, lr=0.000001
  Batch 70: loss=0.7428, grad_norm=0.4799, lr=0.000001
  Batch 75: loss=0.7287, grad_norm=1.7365, lr=0.000001
  Batch 80: loss=0.6023, grad_norm=1.6366, lr=0.000001
  Batch 85: loss=0.6433, grad_norm=1.4651, lr=0.000001
  Batch 90: loss=0.5969, grad_norm=0.5647, lr=0.000001
  Batch 95: loss=0.6849, grad_norm=1.4011, lr=0.000001
  Batch 100: loss=0.6608, grad_norm=0.7426, lr=0.000001
  Batch 105: loss=0.7747, grad_norm=1.1956, lr=0.000001
  Batch 110: loss=0.7373, grad_norm=0.7337, lr=0.000001
  Batch 115: loss=0.6149, grad_norm=1.2790, lr=0.000001
  Batch 120: loss=0.6694, grad_norm=0.9779, lr=0.000001
  Batch 125: loss=0.5215, grad_norm=0.8097, lr=0.000001
  Batch 130: loss=0.6886, grad_norm=2.3614, lr=0.000001
  Batch 135: loss=0.7882, grad_norm=1.3558, lr=0.000001
  Batch 140: loss=0.6651, grad_norm=2.2675, lr=0.000001
  Batch 145: loss=0.7008, grad_norm=1.6527, lr=0.000001
  Batch 150: loss=0.6829, grad_norm=2.0242, lr=0.000001
  Batch 155: loss=0.5736, grad_norm=2.3468, lr=0.000001
  Batch 160: loss=0.4490, grad_norm=1.1188, lr=0.000001
  Batch 165: loss=0.5577, grad_norm=0.7143, lr=0.000001
  Batch 170: loss=0.8048, grad_norm=3.6263, lr=0.000001
  Batch 175: loss=0.7935, grad_norm=2.2312, lr=0.000001
  Batch 180: loss=0.5806, grad_norm=0.9700, lr=0.000001
  Batch 185: loss=0.6712, grad_norm=1.4460, lr=0.000001
  Batch 190: loss=0.6292, grad_norm=1.3721, lr=0.000001
  Batch 195: loss=0.6852, grad_norm=1.2915, lr=0.000001
  Batch 200: loss=0.6167, grad_norm=0.7449, lr=0.000001
  Batch 205: loss=0.7412, grad_norm=1.7004, lr=0.000001
  Batch 210: loss=0.6723, grad_norm=0.7376, lr=0.000001
  Batch 215: loss=0.7768, grad_norm=2.2994, lr=0.000001
  Batch 220: loss=0.6792, grad_norm=1.2014, lr=0.000001
  Batch 225: loss=0.6847, grad_norm=1.4559, lr=0.000001
  Batch 230: loss=0.7800, grad_norm=0.8632, lr=0.000001
  Batch 235: loss=0.6978, grad_norm=1.3430, lr=0.000001
  Batch 240: loss=0.6542, grad_norm=1.1989, lr=0.000001
  Batch 245: loss=0.5767, grad_norm=1.0346, lr=0.000001
  Batch 250: loss=0.6740, grad_norm=1.3864, lr=0.000001
  Batch 255: loss=0.7332, grad_norm=0.8127, lr=0.000001
Validation...
Epoch 11 completed in 2618.52s:
  Train Loss: 0.6747, Train Acc: 0.7467
  Val Loss: 0.7362, Val Acc: 0.7321
  Learning Rate: 0.000001

Epoch 12/20
Training...
Model has 1,982,248 trainable parameters
Computing initial validation metrics...
Initial validation - Loss: 0.7362, Acc: 0.7320
  Batch 5: loss=0.6888, grad_norm=2.6611, lr=0.000001
  Batch 10: loss=0.6924, grad_norm=2.7745, lr=0.000001
  Batch 15: loss=0.6529, grad_norm=1.6629, lr=0.000001
  Batch 20: loss=0.7421, grad_norm=1.8072, lr=0.000001
  Batch 25: loss=0.7761, grad_norm=1.3421, lr=0.000001
  Batch 30: loss=0.6988, grad_norm=1.0303, lr=0.000001
  Batch 35: loss=0.6631, grad_norm=0.4133, lr=0.000001
  Batch 40: loss=0.6647, grad_norm=0.9635, lr=0.000001
  Batch 45: loss=0.7545, grad_norm=0.4749, lr=0.000001
  Batch 50: loss=0.7087, grad_norm=0.4448, lr=0.000001
  Batch 55: loss=0.6870, grad_norm=0.5941, lr=0.000001
  Batch 60: loss=0.6494, grad_norm=0.6033, lr=0.000001
  Batch 65: loss=0.6117, grad_norm=0.6315, lr=0.000001
  Batch 70: loss=0.5549, grad_norm=1.1606, lr=0.000001
  Batch 75: loss=0.5324, grad_norm=0.7174, lr=0.000001
  Batch 80: loss=0.6821, grad_norm=0.8490, lr=0.000001
  Batch 85: loss=0.6470, grad_norm=0.8781, lr=0.000001
  Batch 90: loss=0.6012, grad_norm=0.6608, lr=0.000001
  Batch 95: loss=0.6120, grad_norm=1.6542, lr=0.000001
  Batch 100: loss=0.6534, grad_norm=1.0190, lr=0.000001
  Batch 105: loss=0.5917, grad_norm=1.4237, lr=0.000001
  Batch 110: loss=0.5260, grad_norm=0.4735, lr=0.000001
  Batch 115: loss=0.6738, grad_norm=2.9272, lr=0.000001
  Batch 120: loss=0.5119, grad_norm=0.7042, lr=0.000001
  Batch 125: loss=0.7463, grad_norm=1.8905, lr=0.000001
  Batch 130: loss=0.6241, grad_norm=0.8190, lr=0.000001
  Batch 135: loss=0.6196, grad_norm=2.0094, lr=0.000001
  Batch 140: loss=0.5321, grad_norm=0.9368, lr=0.000001
  Batch 145: loss=0.6154, grad_norm=4.6290, lr=0.000001
  Batch 150: loss=0.7158, grad_norm=1.4826, lr=0.000001
  Batch 155: loss=0.6166, grad_norm=0.9878, lr=0.000001
  Batch 160: loss=0.7168, grad_norm=1.2390, lr=0.000001
  Batch 165: loss=0.6757, grad_norm=2.1681, lr=0.000001
  Batch 170: loss=0.6998, grad_norm=0.8172, lr=0.000001
  Batch 175: loss=0.6278, grad_norm=0.9389, lr=0.000001
  Batch 180: loss=0.7110, grad_norm=1.9359, lr=0.000001
  Batch 185: loss=0.6995, grad_norm=0.6957, lr=0.000001
  Batch 190: loss=0.6364, grad_norm=1.1756, lr=0.000001
  Batch 195: loss=0.7334, grad_norm=1.7946, lr=0.000001
  Batch 200: loss=0.7071, grad_norm=1.1745, lr=0.000001
  Batch 205: loss=0.7014, grad_norm=1.1128, lr=0.000001
  Batch 210: loss=0.7459, grad_norm=0.6738, lr=0.000001
  Batch 215: loss=0.7011, grad_norm=1.1313, lr=0.000001
  Batch 220: loss=0.7653, grad_norm=1.7148, lr=0.000001
  Batch 225: loss=0.6802, grad_norm=0.7687, lr=0.000001
  Batch 230: loss=0.7633, grad_norm=0.7819, lr=0.000001
  Batch 235: loss=0.6406, grad_norm=1.5516, lr=0.000001
  Batch 240: loss=0.7191, grad_norm=1.1522, lr=0.000001
  Batch 245: loss=0.6512, grad_norm=1.0556, lr=0.000001
  Batch 250: loss=0.7077, grad_norm=2.9514, lr=0.000001
  Batch 255: loss=0.7257, grad_norm=2.4185, lr=0.000001
Validation...
Epoch 12 completed in 2642.83s:
  Train Loss: 0.6719, Train Acc: 0.7486
  Val Loss: 0.7361, Val Acc: 0.7334
  Learning Rate: 0.000001

Epoch 13/20
Training...
Model has 1,982,248 trainable parameters
Computing initial validation metrics...
Initial validation - Loss: 0.7360, Acc: 0.7335
  Batch 5: loss=0.7480, grad_norm=3.8076, lr=0.000001
  Batch 10: loss=0.8193, grad_norm=3.3607, lr=0.000001
  Batch 15: loss=0.7054, grad_norm=1.8338, lr=0.000001
  Batch 20: loss=0.6625, grad_norm=1.0575, lr=0.000001
  Batch 25: loss=0.6542, grad_norm=1.8392, lr=0.000001
  Batch 30: loss=0.6362, grad_norm=1.2238, lr=0.000001
  Batch 35: loss=0.6334, grad_norm=1.1746, lr=0.000001
  Batch 40: loss=0.7264, grad_norm=0.7853, lr=0.000001
  Batch 45: loss=0.7578, grad_norm=0.5546, lr=0.000001
  Batch 50: loss=0.6908, grad_norm=0.5165, lr=0.000001
  Batch 55: loss=0.6385, grad_norm=0.4333, lr=0.000001
  Batch 60: loss=0.8012, grad_norm=0.8502, lr=0.000001
  Batch 65: loss=0.5544, grad_norm=0.3832, lr=0.000001
  Batch 70: loss=0.6981, grad_norm=0.6908, lr=0.000001
  Batch 75: loss=0.5566, grad_norm=0.4403, lr=0.000001
  Batch 80: loss=0.5668, grad_norm=1.3289, lr=0.000001
  Batch 85: loss=0.5803, grad_norm=1.0226, lr=0.000001
  Batch 90: loss=0.4949, grad_norm=0.5196, lr=0.000001
  Batch 95: loss=0.7178, grad_norm=0.6099, lr=0.000001
  Batch 100: loss=0.7079, grad_norm=0.7333, lr=0.000001
  Batch 105: loss=0.5623, grad_norm=0.4729, lr=0.000001
  Batch 110: loss=0.5426, grad_norm=1.6567, lr=0.000001
  Batch 115: loss=0.5973, grad_norm=0.8783, lr=0.000001
  Batch 120: loss=0.7233, grad_norm=1.5230, lr=0.000001
  Batch 125: loss=0.5147, grad_norm=0.8679, lr=0.000001
  Batch 130: loss=0.6328, grad_norm=2.9758, lr=0.000001
  Batch 135: loss=0.7225, grad_norm=1.8273, lr=0.000001
  Batch 140: loss=0.7750, grad_norm=1.1156, lr=0.000001
  Batch 145: loss=0.6656, grad_norm=1.0487, lr=0.000001
  Batch 150: loss=0.7005, grad_norm=1.7560, lr=0.000001
  Batch 155: loss=0.5889, grad_norm=1.9053, lr=0.000001
  Batch 160: loss=0.6462, grad_norm=3.4573, lr=0.000001
  Batch 165: loss=0.4059, grad_norm=0.6026, lr=0.000001
  Batch 170: loss=0.6967, grad_norm=0.8901, lr=0.000001
  Batch 175: loss=0.7016, grad_norm=0.9370, lr=0.000001
  Batch 180: loss=0.6564, grad_norm=1.4996, lr=0.000001
  Batch 185: loss=0.7441, grad_norm=2.5668, lr=0.000001
  Batch 190: loss=0.6383, grad_norm=0.8000, lr=0.000001
  Batch 195: loss=0.7529, grad_norm=2.5019, lr=0.000001
  Batch 200: loss=0.6773, grad_norm=2.0782, lr=0.000001
  Batch 205: loss=0.6847, grad_norm=1.4204, lr=0.000001
  Batch 210: loss=0.6338, grad_norm=1.1481, lr=0.000001
  Batch 215: loss=0.6807, grad_norm=1.2641, lr=0.000001
  Batch 220: loss=0.6570, grad_norm=1.0735, lr=0.000001
  Batch 225: loss=0.7399, grad_norm=1.0854, lr=0.000001
  Batch 230: loss=0.6079, grad_norm=2.2884, lr=0.000001
  Batch 235: loss=0.6418, grad_norm=2.6230, lr=0.000001
  Batch 240: loss=0.7066, grad_norm=1.4937, lr=0.000001
  Batch 245: loss=0.5936, grad_norm=1.3940, lr=0.000001
  Batch 250: loss=0.6073, grad_norm=1.5261, lr=0.000001
  Batch 255: loss=0.6926, grad_norm=1.8832, lr=0.000001
Validation...
Epoch 13 completed in 2625.02s:
  Train Loss: 0.6760, Train Acc: 0.7467
  Val Loss: 0.7364, Val Acc: 0.7323
  Learning Rate: 0.000001

Epoch 14/20
Training...
Model has 1,982,248 trainable parameters
Computing initial validation metrics...
Initial validation - Loss: 0.7366, Acc: 0.7316
  Batch 5: loss=0.8694, grad_norm=2.8735, lr=0.000001
  Batch 10: loss=0.7673, grad_norm=2.5873, lr=0.000001
  Batch 15: loss=0.6748, grad_norm=1.5269, lr=0.000001
  Batch 20: loss=0.6696, grad_norm=1.1382, lr=0.000001
  Batch 25: loss=0.6295, grad_norm=1.5236, lr=0.000001
  Batch 30: loss=0.7660, grad_norm=1.0412, lr=0.000001
  Batch 35: loss=0.7068, grad_norm=0.5921, lr=0.000001
  Batch 40: loss=0.8732, grad_norm=1.2064, lr=0.000001
  Batch 45: loss=0.6177, grad_norm=0.3784, lr=0.000001
  Batch 50: loss=0.6351, grad_norm=0.4050, lr=0.000001
  Batch 55: loss=0.7822, grad_norm=0.6474, lr=0.000001
  Batch 60: loss=0.6211, grad_norm=0.8419, lr=0.000001
  Batch 65: loss=0.7508, grad_norm=0.8748, lr=0.000001
  Batch 70: loss=0.6944, grad_norm=0.5091, lr=0.000001
  Batch 75: loss=0.6090, grad_norm=0.7670, lr=0.000001
  Batch 80: loss=0.6790, grad_norm=1.3969, lr=0.000001
  Batch 85: loss=0.5370, grad_norm=1.9551, lr=0.000001
  Batch 90: loss=0.6923, grad_norm=1.7563, lr=0.000001
  Batch 95: loss=0.7574, grad_norm=1.2765, lr=0.000001
  Batch 100: loss=0.7231, grad_norm=1.4438, lr=0.000001
  Batch 105: loss=0.6445, grad_norm=0.5162, lr=0.000001
  Batch 110: loss=0.6599, grad_norm=1.3818, lr=0.000001
  Batch 115: loss=0.5591, grad_norm=1.2084, lr=0.000001
  Batch 120: loss=0.6246, grad_norm=1.9236, lr=0.000001
  Batch 125: loss=0.6089, grad_norm=1.6214, lr=0.000001
  Batch 130: loss=0.7195, grad_norm=1.8700, lr=0.000001
  Batch 135: loss=0.7053, grad_norm=1.1542, lr=0.000001
  Batch 140: loss=0.7390, grad_norm=1.6302, lr=0.000001
  Batch 145: loss=0.6284, grad_norm=2.8564, lr=0.000001
  Batch 150: loss=0.7081, grad_norm=1.4374, lr=0.000001
  Batch 155: loss=0.6988, grad_norm=0.9035, lr=0.000001
  Batch 160: loss=0.5511, grad_norm=1.3219, lr=0.000001
  Batch 165: loss=0.7405, grad_norm=0.9500, lr=0.000001
  Batch 170: loss=0.6629, grad_norm=1.1164, lr=0.000001
  Batch 175: loss=0.7859, grad_norm=1.1164, lr=0.000001
  Batch 180: loss=0.7473, grad_norm=2.0656, lr=0.000001
  Batch 185: loss=0.6070, grad_norm=2.4514, lr=0.000001
  Batch 190: loss=0.7053, grad_norm=1.7429, lr=0.000001
  Batch 195: loss=0.6291, grad_norm=1.5748, lr=0.000001
  Batch 200: loss=0.7547, grad_norm=2.8083, lr=0.000001
  Batch 205: loss=0.6999, grad_norm=0.8173, lr=0.000001
  Batch 210: loss=0.7451, grad_norm=0.9858, lr=0.000001
  Batch 215: loss=0.6337, grad_norm=1.1961, lr=0.000001
  Batch 220: loss=0.6099, grad_norm=0.7677, lr=0.000001
  Batch 225: loss=0.6138, grad_norm=2.4175, lr=0.000001
  Batch 230: loss=0.8469, grad_norm=1.3851, lr=0.000001
  Batch 235: loss=0.7540, grad_norm=3.5230, lr=0.000001
  Batch 240: loss=0.7011, grad_norm=1.8398, lr=0.000001
  Batch 245: loss=0.6955, grad_norm=1.7578, lr=0.000001
  Batch 250: loss=0.7061, grad_norm=1.2885, lr=0.000001
  Batch 255: loss=0.6535, grad_norm=1.0285, lr=0.000001
Validation...
Epoch 14 completed in 2667.35s:
  Train Loss: 0.6803, Train Acc: 0.7449
  Val Loss: 0.7366, Val Acc: 0.7312
  Learning Rate: 0.000001

Epoch 15/20
Training...
Model has 1,982,248 trainable parameters
Computing initial validation metrics...
Initial validation - Loss: 0.7368, Acc: 0.7308
  Batch 5: loss=0.7110, grad_norm=2.1378, lr=0.000001
  Batch 10: loss=0.7268, grad_norm=1.6235, lr=0.000001
  Batch 15: loss=0.7243, grad_norm=1.3002, lr=0.000001
  Batch 20: loss=0.6346, grad_norm=0.8515, lr=0.000001
  Batch 25: loss=0.7426, grad_norm=1.0370, lr=0.000001
  Batch 30: loss=0.6996, grad_norm=1.1267, lr=0.000001
  Batch 35: loss=0.6483, grad_norm=0.9922, lr=0.000001
  Batch 40: loss=0.8916, grad_norm=1.1834, lr=0.000001
  Batch 45: loss=0.6569, grad_norm=1.0408, lr=0.000001
  Batch 50: loss=0.6371, grad_norm=0.6300, lr=0.000001
  Batch 55: loss=0.7667, grad_norm=0.6101, lr=0.000001
  Batch 60: loss=0.7096, grad_norm=0.4477, lr=0.000001
  Batch 65: loss=0.7977, grad_norm=0.8552, lr=0.000001
  Batch 70: loss=0.7989, grad_norm=1.0910, lr=0.000001
  Batch 75: loss=0.6636, grad_norm=0.7345, lr=0.000001
  Batch 80: loss=0.8659, grad_norm=0.8933, lr=0.000001
  Batch 85: loss=0.6582, grad_norm=1.7930, lr=0.000001
  Batch 90: loss=0.7765, grad_norm=1.0611, lr=0.000001
  Batch 95: loss=0.7603, grad_norm=0.7853, lr=0.000001
  Batch 100: loss=0.6582, grad_norm=1.1603, lr=0.000001
  Batch 105: loss=0.6806, grad_norm=0.6902, lr=0.000001
  Batch 110: loss=0.6874, grad_norm=1.2262, lr=0.000001
  Batch 115: loss=0.6877, grad_norm=1.5357, lr=0.000001
  Batch 120: loss=0.5944, grad_norm=1.3556, lr=0.000001
  Batch 125: loss=0.6631, grad_norm=0.7809, lr=0.000001
  Batch 130: loss=0.5469, grad_norm=1.1474, lr=0.000001
  Batch 135: loss=0.6469, grad_norm=1.8402, lr=0.000001
  Batch 140: loss=0.7893, grad_norm=0.7230, lr=0.000001
  Batch 145: loss=0.6222, grad_norm=0.8596, lr=0.000001
  Batch 150: loss=0.7344, grad_norm=0.6770, lr=0.000001
  Batch 155: loss=0.6825, grad_norm=3.0502, lr=0.000001
  Batch 160: loss=0.6463, grad_norm=1.1410, lr=0.000001
  Batch 165: loss=0.7272, grad_norm=0.9431, lr=0.000001
  Batch 170: loss=0.5648, grad_norm=1.3438, lr=0.000001
  Batch 175: loss=0.7477, grad_norm=1.5163, lr=0.000001
  Batch 180: loss=0.8885, grad_norm=3.3431, lr=0.000001
  Batch 185: loss=0.5717, grad_norm=1.3410, lr=0.000001
  Batch 190: loss=0.7925, grad_norm=1.1429, lr=0.000001
  Batch 195: loss=0.6333, grad_norm=0.5793, lr=0.000001
  Batch 200: loss=0.6943, grad_norm=2.6018, lr=0.000001
  Batch 205: loss=0.7066, grad_norm=1.2343, lr=0.000001
  Batch 210: loss=0.6996, grad_norm=1.4718, lr=0.000001
  Batch 215: loss=0.7064, grad_norm=1.0561, lr=0.000001
  Batch 220: loss=0.7439, grad_norm=0.8348, lr=0.000001
  Batch 225: loss=0.5876, grad_norm=0.9046, lr=0.000001
  Batch 230: loss=0.8056, grad_norm=1.3915, lr=0.000001
  Batch 235: loss=0.5850, grad_norm=2.0420, lr=0.000001
  Batch 240: loss=0.5800, grad_norm=1.8905, lr=0.000001
  Batch 245: loss=0.5445, grad_norm=1.5137, lr=0.000001
  Batch 250: loss=0.6026, grad_norm=0.5919, lr=0.000001
  Batch 255: loss=0.5857, grad_norm=1.1372, lr=0.000001
Validation...
Epoch 15 completed in 2847.11s:
  Train Loss: 0.6878, Train Acc: 0.7421
  Val Loss: 0.7368, Val Acc: 0.7311
  Learning Rate: 0.000001
  Checkpoint saved: ttt_experiments/basic_ttt_20250609_020007/checkpoints/ckpt-3

Epoch 16/20
Training...
Model has 1,982,248 trainable parameters
Computing initial validation metrics...
Initial validation - Loss: 0.7370, Acc: 0.7307
  Batch 5: loss=0.7049, grad_norm=3.0336, lr=0.000001
  Batch 10: loss=0.7795, grad_norm=2.0226, lr=0.000001
  Batch 15: loss=0.7017, grad_norm=1.7972, lr=0.000001
  Batch 20: loss=0.6277, grad_norm=1.0654, lr=0.000001
  Batch 25: loss=0.6925, grad_norm=1.0286, lr=0.000001
  Batch 30: loss=0.6350, grad_norm=0.6487, lr=0.000001
  Batch 35: loss=0.7528, grad_norm=1.0759, lr=0.000001
  Batch 40: loss=0.7649, grad_norm=0.5918, lr=0.000001
  Batch 45: loss=0.7470, grad_norm=0.5069, lr=0.000001
  Batch 50: loss=0.7226, grad_norm=0.7433, lr=0.000001
  Batch 55: loss=0.7774, grad_norm=0.5020, lr=0.000001
  Batch 60: loss=0.6452, grad_norm=0.4467, lr=0.000001
  Batch 65: loss=0.5904, grad_norm=1.1595, lr=0.000001
  Batch 70: loss=0.5805, grad_norm=0.9338, lr=0.000001
  Batch 75: loss=0.8353, grad_norm=1.8529, lr=0.000001
  Batch 80: loss=0.7421, grad_norm=1.0866, lr=0.000001
  Batch 85: loss=0.6762, grad_norm=0.4855, lr=0.000001
  Batch 90: loss=0.6288, grad_norm=1.0135, lr=0.000001
  Batch 95: loss=0.8217, grad_norm=1.4602, lr=0.000001
  Batch 100: loss=0.5381, grad_norm=0.3508, lr=0.000001
  Batch 105: loss=0.6282, grad_norm=0.8134, lr=0.000001
  Batch 110: loss=0.6821, grad_norm=1.2907, lr=0.000001
  Batch 115: loss=0.7192, grad_norm=1.0410, lr=0.000001
  Batch 120: loss=0.6990, grad_norm=2.4133, lr=0.000001
  Batch 125: loss=0.7424, grad_norm=1.0638, lr=0.000001
  Batch 130: loss=0.5931, grad_norm=0.7388, lr=0.000001
  Batch 135: loss=0.5825, grad_norm=1.1837, lr=0.000001
  Batch 140: loss=0.7308, grad_norm=2.8665, lr=0.000001
  Batch 145: loss=0.6469, grad_norm=1.1616, lr=0.000001
  Batch 150: loss=0.7788, grad_norm=1.5195, lr=0.000001
  Batch 155: loss=0.4485, grad_norm=1.4415, lr=0.000001
  Batch 160: loss=0.7048, grad_norm=1.1840, lr=0.000001
  Batch 165: loss=0.6051, grad_norm=1.0074, lr=0.000001
  Batch 170: loss=0.5986, grad_norm=2.5489, lr=0.000001
  Batch 175: loss=0.7780, grad_norm=1.7923, lr=0.000001
  Batch 180: loss=0.7346, grad_norm=2.4956, lr=0.000001
  Batch 185: loss=0.7118, grad_norm=1.5747, lr=0.000001
  Batch 190: loss=0.6469, grad_norm=1.3340, lr=0.000001
  Batch 195: loss=0.6209, grad_norm=0.7776, lr=0.000001
  Batch 200: loss=0.6624, grad_norm=2.4225, lr=0.000001
  Batch 205: loss=0.6467, grad_norm=1.0888, lr=0.000001
  Batch 210: loss=0.5687, grad_norm=1.3756, lr=0.000001
  Batch 215: loss=0.6481, grad_norm=1.0913, lr=0.000001
  Batch 220: loss=0.6575, grad_norm=1.3741, lr=0.000001
  Batch 225: loss=0.6858, grad_norm=2.2032, lr=0.000001
  Batch 230: loss=0.6253, grad_norm=1.5244, lr=0.000001
  Batch 235: loss=0.6339, grad_norm=2.8001, lr=0.000001
  Batch 240: loss=0.5497, grad_norm=1.5584, lr=0.000001
  Batch 245: loss=0.7732, grad_norm=2.3992, lr=0.000001
  Batch 250: loss=0.6913, grad_norm=2.0923, lr=0.000001
  Batch 255: loss=0.7570, grad_norm=0.7690, lr=0.000001
Validation...
Epoch 16 completed in 2914.60s:
  Train Loss: 0.6690, Train Acc: 0.7504
  Val Loss: 0.7369, Val Acc: 0.7310
  Learning Rate: 0.000001

Epoch 17/20
Training...
Model has 1,982,248 trainable parameters
Computing initial validation metrics...
Initial validation - Loss: 0.7366, Acc: 0.7314
  Batch 5: loss=0.7834, grad_norm=3.3897, lr=0.000001
  Batch 10: loss=0.8021, grad_norm=3.2898, lr=0.000001
  Batch 15: loss=0.6616, grad_norm=1.4561, lr=0.000001
  Batch 20: loss=0.8031, grad_norm=1.6957, lr=0.000001
  Batch 25: loss=0.7390, grad_norm=1.2082, lr=0.000001
  Batch 30: loss=0.6705, grad_norm=0.7854, lr=0.000001
  Batch 35: loss=0.6366, grad_norm=0.8114, lr=0.000001
  Batch 40: loss=0.7551, grad_norm=0.9675, lr=0.000001
  Batch 45: loss=0.8019, grad_norm=0.9808, lr=0.000001
  Batch 50: loss=0.8178, grad_norm=0.8153, lr=0.000001
  Batch 55: loss=0.7454, grad_norm=0.6085, lr=0.000001
  Batch 60: loss=0.6481, grad_norm=0.6341, lr=0.000001
  Batch 65: loss=0.7856, grad_norm=0.6166, lr=0.000001
  Batch 70: loss=0.6031, grad_norm=0.5667, lr=0.000001
  Batch 75: loss=0.5603, grad_norm=0.7865, lr=0.000001
  Batch 80: loss=0.5317, grad_norm=0.9441, lr=0.000001
  Batch 85: loss=0.7777, grad_norm=1.9068, lr=0.000001
  Batch 90: loss=0.7738, grad_norm=1.2992, lr=0.000001
  Batch 95: loss=0.6338, grad_norm=1.0512, lr=0.000001
  Batch 100: loss=0.7073, grad_norm=1.8267, lr=0.000001
  Batch 105: loss=0.6782, grad_norm=1.0257, lr=0.000001
  Batch 110: loss=0.5282, grad_norm=1.2891, lr=0.000001
  Batch 115: loss=0.6538, grad_norm=1.9542, lr=0.000001
  Batch 120: loss=0.5855, grad_norm=2.3726, lr=0.000001
  Batch 125: loss=0.6942, grad_norm=1.4868, lr=0.000001
  Batch 130: loss=0.5411, grad_norm=1.5211, lr=0.000001
  Batch 135: loss=0.6909, grad_norm=1.5784, lr=0.000001
  Batch 140: loss=0.6165, grad_norm=2.2156, lr=0.000001
  Batch 145: loss=0.5318, grad_norm=3.1496, lr=0.000001
  Batch 150: loss=0.6668, grad_norm=2.0620, lr=0.000001
  Batch 155: loss=0.6476, grad_norm=1.4471, lr=0.000001
  Batch 160: loss=0.6848, grad_norm=1.0492, lr=0.000001
  Batch 165: loss=0.6133, grad_norm=2.6991, lr=0.000001
  Batch 170: loss=0.7268, grad_norm=1.5145, lr=0.000001
  Batch 175: loss=0.5706, grad_norm=1.8360, lr=0.000001
  Batch 180: loss=0.6860, grad_norm=0.5672, lr=0.000001
  Batch 185: loss=0.6330, grad_norm=1.4604, lr=0.000001
  Batch 190: loss=0.6801, grad_norm=0.6902, lr=0.000001
  Batch 195: loss=0.6038, grad_norm=1.1683, lr=0.000001
  Batch 200: loss=0.6497, grad_norm=0.8922, lr=0.000001
  Batch 205: loss=0.6657, grad_norm=0.9043, lr=0.000001
  Batch 210: loss=0.4838, grad_norm=0.9850, lr=0.000001
  Batch 215: loss=0.7722, grad_norm=1.7184, lr=0.000001
  Batch 220: loss=0.6710, grad_norm=1.2245, lr=0.000001
  Batch 225: loss=0.7612, grad_norm=1.6420, lr=0.000001
  Batch 230: loss=0.7705, grad_norm=2.0734, lr=0.000001
  Batch 235: loss=0.6835, grad_norm=1.5055, lr=0.000001
  Batch 240: loss=0.7715, grad_norm=2.1721, lr=0.000001
  Batch 245: loss=0.6956, grad_norm=1.7946, lr=0.000001
  Batch 250: loss=0.6137, grad_norm=1.4729, lr=0.000001
  Batch 255: loss=0.6496, grad_norm=0.9020, lr=0.000001
Validation...
Epoch 17 completed in 2907.41s:
  Train Loss: 0.6803, Train Acc: 0.7449
  Val Loss: 0.7359, Val Acc: 0.7320
  Learning Rate: 0.000001

Epoch 18/20
Training...
Model has 1,982,248 trainable parameters
Computing initial validation metrics...
Initial validation - Loss: 0.7360, Acc: 0.7319
  Batch 5: loss=0.7516, grad_norm=2.7084, lr=0.000001
  Batch 10: loss=0.7450, grad_norm=2.1890, lr=0.000001
  Batch 15: loss=0.6945, grad_norm=2.1025, lr=0.000001
  Batch 20: loss=0.6746, grad_norm=0.6849, lr=0.000001
  Batch 25: loss=0.7020, grad_norm=1.4616, lr=0.000001
  Batch 30: loss=0.5946, grad_norm=0.7277, lr=0.000001
  Batch 35: loss=0.6726, grad_norm=0.6190, lr=0.000001
  Batch 40: loss=0.6785, grad_norm=0.8954, lr=0.000001
